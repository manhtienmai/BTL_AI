# -*- coding: utf-8 -*-
"""Non-linear Regression

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aHyWJi9XF5AlyfrW7rk-TxByECUkrNal
"""

#Cau 2
class RegressionModel(object):
    """
    A neural network model for approximating a function that maps from real
    numbers to real numbers. The network should be sufficiently large to be able
    to approximate sin(x) on the interval [-2pi, 2pi] to reasonable precision.
    """
    def __init__(self):
        # Initialize your model parameters here
        "*** YOUR CODE HERE ***"
        #initialize weights and biases
        self.w1 = nn.Parameter(1, 512)
        self.b1 = nn.Parameter(1, 512)
        self.w2 = nn.Parameter(512, 1)
        self.b2 = nn.Parameter(1, 1)


    def run(self, x):
        """
        Runs the model for a batch of examples.

        Inputs:
            x: a node with shape (batch_size x 1)
        Returns:
            A node with shape (batch_size x 1) containing predicted y-values
        """
        "*** YOUR CODE HERE ***"
        x = nn.Linear(x, self.w1)
        x = nn.AddBias(x, self.b1)
        x = nn.ReLU(x)
        x2 = nn.AddBias(nn.Linear(x, self.w2), self.b2)
        return x2
    def get_loss(self, x, y):
        """
        Computes the loss for a batch of examples.

        Inputs:
            x: a node with shape (batch_size x 1)
            y: a node with shape (batch_size x 1), containing the true y-values
                to be used for training
        Returns: a loss node
        """
        "*** YOUR CODE HERE ***"
        predicted_value = self.run(x)
        return nn.SquareLoss(predicted_value, y)
    def train(self, dataset):
        """
        Trains the model.
        """
        "*** YOUR CODE HERE ***"
        learning_rate = 0.05
        batch_size = 200

        while True:
            for x, y in dataset.iterate_once(batch_size):
                loss = self.get_loss(x, y)
                loss_scalar = nn.as_scalar(loss)
                if loss_scalar <= 0.02:
                    return
                gradients = nn.gradients(loss, [self.w1, self.b1, self.w2, self.b2])
                self.w1.update(gradients[0], -learning_rate)
                self.b1.update(gradients[1], -learning_rate)
                self.w2.update(gradients[2], -learning_rate)
                self.b2.update(gradients[3], -learning_rate)